# === MEMORY OPTIMIZATION ENV VARS ===
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"  # Gi·∫£m memory fragmentation

# Load model v√† tokenizer
import torch
from unsloth import FastLanguageModel


model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Qwen3-4B-Instruct-2507",
    max_seq_length = 512,

    full_finetuning = False,
    load_in_4bit = False,  # MUST be False for QAT - load full precision weights
    dtype = torch.bfloat16,  # Use bf16 for memory efficiency
    
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 8, # Gi·∫£m t·ª´ 16 xu·ªëng 8 ƒë·ªÉ ti·∫øt ki·ªám VRAM
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,  # Gi·∫£m theo t·ª∑ l·ªá v·ªõi r (alpha/r = 2)
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    qat_scheme = "int4",
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

        
import re
from unsloth.chat_templates import get_chat_template
from ChatBotSynthetic.training.sft import DatasetLoader
dataset_config = {
        "dataset": {
            "train_path": "ChatBotSynthetic/data/train.jsonl",
            "validation_path": "ChatBotSynthetic/data/validation.jsonl",
            "format": "json",  # Note: use "json" not "jsonl" for HF datasets
            "message_field": "messages",
            "text_field": "text"
        }
    }
loader = DatasetLoader(dataset_config, tokenizer)
train_dataset = loader.prepare_dataset(split="train")
val_dataset = loader.prepare_dataset(split="validation")

SYSTEM_PROMPT = "B·∫°n l√† nh√¢n vi√™n CSKH Heineken Vietnam ƒëang h·ªó tr·ª£ tr·ª£ kh√°ch h√†ng theo nh·ªØng quy tr√¨nh c√≥ s·∫µn."

def format_multi_turn_grpo(example):
    """
    Format dataset theo ƒë√∫ng ƒë·ªãnh d·∫°ng c·ªßa GRPO training v·ªõi reasoning_content v√† tool_calls
    """
    import json
    from datetime import datetime, date
    
    def serialize_tool_args(obj):
        """Helper function ƒë·ªÉ serialize c√°c object kh√¥ng ph·∫£i JSON standard"""
        if isinstance(obj, (datetime, date)):
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {k: serialize_tool_args(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [serialize_tool_args(item) for item in obj]
        elif obj is None:
            return None
        else:
            return obj
    
    messages = example["messages"]
    
    # X·ª≠ l√Ω t·ª´ng message ƒë·ªÉ format l·∫°i content
    formatted_messages = []
    for msg in messages:
        role = msg.get("role")
        content = msg.get("content", "")
        reasoning = msg.get("reasoning_content", "")
        tool_calls = msg.get("tool_calls")
        
        # X√¢y d·ª±ng n·ªôi dung ho√†n ch·ªânh cho assistant
        if role == "assistant":
            full_content = ""
            
            # Th√™m ph·∫ßn reasoning v√†o th·∫ª <think> n·∫øu c√≥
            if reasoning and reasoning.strip():
                full_content += f"<think>{reasoning}</think>"
            
            # Th√™m ph·∫ßn content ch√≠nh (response text)
            if content and content.strip():
                if full_content:  # N·∫øu ƒë√£ c√≥ reasoning, th√™m xu·ªëng d√≤ng
                    full_content += "\n"
                full_content += content
            
            # X·ª≠ l√Ω tool_calls n·∫øu c√≥
            if tool_calls:
                for tool_call in tool_calls:
                    tool_name = tool_call.get("name", "")
                    tool_args = tool_call.get("arguments", {})
                    
                    # Serialize tool_args ƒë·ªÉ x·ª≠ l√Ω datetime v√† c√°c object kh√°c
                    serialized_args = serialize_tool_args(tool_args)
                    
                    # T·∫°o chu·ªói tool call
                    tool_call_str = f"\n<tool_call>\n{json.dumps({'name': tool_name, 'arguments': serialized_args}, ensure_ascii=False, indent=2)}\n</tool_call>"
                    full_content += tool_call_str
            
            formatted_messages.append({
                "role": role,
                "content": full_content.strip() if full_content.strip() else ""
            })
        else:
            # Gi·ªØ nguy√™n c√°c message kh√¥ng ph·∫£i assistant
            formatted_messages.append({
                "role": role,
                "content": content
            })
    
    # Ki·ªÉm tra message cu·ªëi c√πng ph·∫£i l√† assistant
    if not formatted_messages or formatted_messages[-1]["role"] != "assistant":
        return {"prompt": None, "answer": None}
    
    # L·∫•y n·ªôi dung assistant cu·ªëi c√πng l√†m answer
    last_assistant_content = formatted_messages[-1]["content"]
    
    # --- X√¢y d·ª±ng Prompt (L·ªãch s·ª≠ h·ªôi tho·∫°i) ---
    prompt_history = []
    
    # Th√™m system prompt n·∫øu ch∆∞a c√≥
    if formatted_messages[0]["role"] != "system":
        prompt_history.append({"role": "system", "content": SYSTEM_PROMPT})
    
    # Th√™m t·∫•t c·∫£ c√°c tin nh·∫Øn TR·ª™ tin nh·∫Øn assistant cu·ªëi c√πng v√†o prompt
    prompt_history.extend(formatted_messages[:-1])
    
    # T·∫°o dict m·ªõi v·ªõi c√°c field c·∫ßn thi·∫øt
    result = {
        "prompt": prompt_history,
        "answer": last_assistant_content
    }

    # X√≥a c√°c key procedure_id, procedure_name, procedure_purpose, procedure_main_flow, procedure_edge_cases, messages ƒëi
    example.pop("procedure_id", None)
    example.pop("procedure_name", None)
    example.pop("procedure_purpose", None)
    example.pop("procedure_main_flow", None)
    example.pop("procedure_edge_cases", None)
    example.pop("messages", None)
    example.pop("text", None)
    
    return result

from datasets import load_dataset
train_dataset = train_dataset.map(format_multi_turn_grpo, batched=False)
val_dataset = val_dataset.map(format_multi_turn_grpo, batched=False)

import re

RESPONSE_PATTERN = (
        r"^<think>"  # M·ªü th·∫ª think
        r"\s*T√¨nh hu·ªëng:\s*.+?"  # T√¨nh hu·ªëng (b·∫Øt bu·ªôc c√≥ n·ªôi dung)
        r"\s*\n\s*Quy tr√¨nh:\s*(?:kh√¥ng x√°c ƒë·ªãnh|kh√¥ng li√™n quan|.+?)"  # Quy tr√¨nh
        r"\s*\n\s*B∆∞·ªõc:\s*(?:(?:\d+\s*-\s*[^\n]+)|(?:ngo·∫°i l·ªá\s*-\s*[^\n]+)|)"  # B∆∞·ªõc
        r"\s*\n\s*Th√¥ng tin c√≥:\s*.+?"  # Th√¥ng tin c√≥
        r"\s*\n\s*Th√¥ng tin c·∫ßn th√™m:\s*.+?"  # Th√¥ng tin c·∫ßn th√™m
        r"\s*\n\s*H√†nh ƒë·ªông:\s*.+?"  # H√†nh ƒë·ªông
        r"\s*</think>"  # ƒê√≥ng th·∫ª think
        r"\s*(?:"  # Sau think c√≥ th·ªÉ l√†:
            r"<tool_call>.*?</tool_call>"  # Tool call
            r"|"  # HO·∫∂C
            r".+"  # Response text th√¥ng th∆∞·ªùng
        r")"
    )

def match_response_pattern(res):
    return re.match(RESPONSE_PATTERN, res, re.DOTALL | re.IGNORECASE)

def match_format_exactly(prompts, completions, *args, **kwargs):
    """
    Pattern cho format:
    <think>
    T√¨nh hu·ªëng: ...
    Quy tr√¨nh: ...
    B∆∞·ªõc: ...
    Th√¥ng tin c√≥: ...
    Th√¥ng tin c·∫ßn th√™m: ...
    H√†nh ƒë·ªông: ...
    </think>
    [N·ªôi dung tr·∫£ l·ªùi HO·∫∂C <tool_call>...</tool_call>]
    """
    
    responses = [c[0] for c in completions]
    scores = []
    
    for res in responses:
        if match_response_pattern(res):
            scores.append(1.0)
        else:
            scores.append(0.0)
    return scores

def filter_valid_format(example):
    # L∆ØU √ù: D·ª±a tr√™n code test c·ªßa b·∫°n (original["answer"]), 
    # t√¥i gi·∫£ ƒë·ªãnh n·ªôi dung c·∫ßn ki·ªÉm tra n·∫±m ·ªü c·ªôt 'answer'.
    # N·∫øu dataset sau khi format_multi_turn_grpo l∆∞u ·ªü c·ªôt kh√°c (vd: 'completion'),
    # h√£y ƒë·ªïi 'answer' th√†nh t√™n c·ªôt ƒë√≥.
    content = example.get("answer", "") 
    
    return match_response_pattern(content) is not None
    
# Ki·ªÉm tra s·ªë l∆∞·ª£ng ban ƒë·∫ßu
print(f"Original Train Size: {len(train_dataset)}")
print(f"Original Val Size: {len(val_dataset)}")

# Filter train dataset
train_dataset_filtered = train_dataset.filter(filter_valid_format)

# Filter val dataset
val_dataset_filtered = val_dataset.filter(filter_valid_format)

# 4. K·∫øt qu·∫£
print("-" * 30)
print(f"Filtered Train Size: {len(train_dataset_filtered)} (Removed: {len(train_dataset) - len(train_dataset_filtered)})")
print(f"Filtered Val Size:   {len(val_dataset_filtered)} (Removed: {len(val_dataset) - len(val_dataset_filtered)})")

# C·∫≠p nh·∫≠t l·∫°i bi·∫øn ch√≠nh n·∫øu c·∫ßn
train_dataset = train_dataset_filtered
val_dataset = val_dataset_filtered

import os
from dotenv import load_dotenv
load_dotenv()
import re
import logging
import os
from typing import List, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

from openai import OpenAI
from pydantic import BaseModel, Field, field_validator

# --- C·∫•u h√¨nh Logger ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Kh·ªüi t·∫°o Client (ƒê·∫£m b·∫£o ƒë√£ set bi·∫øn m√¥i tr∆∞·ªùng OPENAI_API_KEY)
# Trong m√¥i tr∆∞·ªùng training, n√™n kh·ªüi t·∫°o client b√™n ngo√†i function n·∫øu c√≥ th·ªÉ ƒë·ªÉ tr√°nh init l·∫°i nhi·ªÅu l·∫ßn
client = OpenAI()

# --- 1. ƒê·ªãnh nghƒ©a Schema Output (Gi·ªØ nguy√™n) ---
class QualityScore(BaseModel):
    """C·∫•u tr√∫c ƒëi·ªÉm s·ªë tr·∫£ v·ªÅ t·ª´ LLM Judge."""
    reasoning: str = Field(
        description="Gi·∫£i th√≠ch ng·∫Øn g·ªçn (d∆∞·ªõi 50 t·ª´) t·∫°i sao l·∫°i cho ƒëi·ªÉm n√†y. "
                    "T·∫≠p trung v√†o s·ª± sai l·ªách gi·ªØa suy nghƒ© v√† c√¢u tr·∫£ l·ªùi."
    )
    score: float = Field(
        description="ƒêi·ªÉm ch·∫•t l∆∞·ª£ng t·ª´ 1.0 ƒë·∫øn 5.0. 1.0 l√† t·ªá nh·∫•t, 5.0 l√† ho√†n h·∫£o."
    )

    @field_validator('score')
    def check_score_range(cls, v):
        if v < 1.0: return 1.0
        if v > 5.0: return 5.0
        return v

# --- 2. Helper g·ªçi OpenAI cho t·ª´ng item ---
def _call_judge_single(item: dict) -> float:
    """H√†m worker ƒë·ªÉ x·ª≠ l√Ω t·ª´ng request (ƒë·ªÉ ch·∫°y song song)."""
    try:
        completion = client.beta.chat.completions.parse(
            model="gpt-4.1-mini", # Model r·∫ª v√† nhanh cho reward function
            temperature=0.0,
            messages=[
                {
                    "role": "system",
                    "content": f"""
B·∫°n l√† Judge chuy√™n nghi·ªáp ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng AI CSKH Heineken Vi·ªát Nam.

### NHI·ªÜM V·ª§
Ch·∫•m ƒëi·ªÉm t·ª´ 0.0 ƒë·∫øn 5.0 d·ª±a tr√™n 3 ti√™u ch√≠:
1. **Format**: C√≥ tu√¢n th·ªß ƒë√∫ng c·∫•u tr√∫c 6 tr∆∞·ªùng kh√¥ng?
2. **Logic**: Suy lu·∫≠n c√≥ h·ª£p l√Ω v√† nh·∫•t qu√°n kh√¥ng?
3. **Accuracy**: C√¢u tr·∫£ l·ªùi c√≥ kh·ªõp Ground Truth kh√¥ng?

### C·∫§U TR√öC FORMAT B·∫ÆT BU·ªòC
```
<think>
T√¨nh hu·ªëng: [m√¥ t·∫£]
Quy tr√¨nh: [t√™n quy tr√¨nh ho·∫∑c "kh√¥ng x√°c ƒë·ªãnh"]
B∆∞·ªõc: [s·ªë - m√¥ t·∫£ ho·∫∑c "ngo·∫°i l·ªá - ..."]
Th√¥ng tin c√≥: [li·ªát k√™]
Th√¥ng tin c·∫ßn th√™m: [li·ªát k√™ ho·∫∑c "Kh√¥ng"]
H√†nh ƒë·ªông: [h√†nh ƒë·ªông c·ª• th·ªÉ]
</think>
[C√¢u tr·∫£ l·ªùi ho·∫∑c <tool_call>]
```

### THANG ƒêI·ªÇM CHI TI·∫æT

**5.0 ƒëi·ªÉm (Xu·∫•t s·∫Øc)**
- ‚úÖ Format ho√†n h·∫£o (ƒë√£ ƒë∆∞·ª£c pre-check)
- ‚úÖ Logic r√µ r√†ng, ch√≠nh x√°c:
  * T√¨nh hu·ªëng ƒë∆∞·ª£c m√¥ t·∫£ ƒë√∫ng v·ªõi c√¢u h·ªèi
  * Quy tr√¨nh v√† B∆∞·ªõc x√°c ƒë·ªãnh ch√≠nh x√°c
  * Th√¥ng tin c√≥/c·∫ßn li·ªát k√™ ƒë·∫ßy ƒë·ªß v√† logic
  * H√†nh ƒë·ªông h·ª£p l√Ω (g·ªçi tool ƒë√∫ng tham s·ªë HO·∫∂C tr·∫£ l·ªùi tr·ª±c ti·∫øp)
- ‚úÖ C√¢u tr·∫£ l·ªùi kh·ªõp 100% v·ªõi Ground Truth (v·ªÅ √Ω nghƒ©a, cho ph√©p di·ªÖn ƒë·∫°t kh√°c)

**4.0 - 4.5 ƒëi·ªÉm (T·ªët)**
- ‚úÖ Format ƒë√∫ng
- ‚úÖ Logic t·ªët nh∆∞ng c√≥ sai s√≥t NH·ªé:
  * T√¨nh hu·ªëng ƒë√∫ng nh∆∞ng m√¥ t·∫£ ch∆∞a ƒë·∫ßy ƒë·ªß
  * B∆∞·ªõc x√°c ƒë·ªãnh ƒë√∫ng h∆∞·ªõng nh∆∞ng ch∆∞a ch√≠nh x√°c 100%
  * Th√¥ng tin c√≥/c·∫ßn thi·∫øu 1-2 chi ti·∫øt kh√¥ng quan tr·ªçng
- ‚úÖ C√¢u tr·∫£ l·ªùi ƒë√∫ng h∆∞·ªõng, sai l·ªách nh·ªè (<10% n·ªôi dung)

**3.0 - 3.5 ƒëi·ªÉm (Trung b√¨nh)**
- ‚úÖ Format ƒë√∫ng
- ‚ö†Ô∏è Logic c√≥ v·∫•n ƒë·ªÅ TRUNG B√åNH:
  * X√°c ƒë·ªãnh Quy tr√¨nh/B∆∞·ªõc SAI nh∆∞ng v·∫´n trong ph·∫°m vi li√™n quan
  * Th√¥ng tin c√≥/c·∫ßn li·ªát k√™ THI·∫æU ho·∫∑c D∆Ø th·ª´a ƒë√°ng k·ªÉ
  * H√†nh ƒë·ªông ƒë√∫ng nh∆∞ng l√Ω do kh√¥ng r√µ r√†ng
- ‚ö†Ô∏è C√¢u tr·∫£ l·ªùi ƒë√∫ng 60-80% n·ªôi dung

**2.0 - 2.5 ƒëi·ªÉm (Y·∫øu)**
- ‚úÖ Format ƒë√∫ng
- ‚ùå Logic SAI NGHI√äM TR·ªåNG:
  * T√¨nh hu·ªëng x√°c ƒë·ªãnh SAI HO√ÄN TO√ÄN
  * Quy tr√¨nh kh√¥ng li√™n quan ƒë·∫øn c√¢u h·ªèi
  * Suy lu·∫≠n m·ªôt ƒë·∫±ng, h√†nh ƒë·ªông m·ªôt n·∫ªo (m√¢u thu·∫´n)
  * Copy-paste ho·∫∑c ƒëi·ªÅn b·ª´a v√†o c√°c tr∆∞·ªùng
- ‚ùå C√¢u tr·∫£ l·ªùi sai 40-60%

**1.0 - 1.5 ƒëi·ªÉm (R·∫•t y·∫øu)**
- ‚úÖ Format ƒë√∫ng
- ‚ùå Logic ho√†n to√†n v√¥ nghƒ©a ho·∫∑c kh√¥ng li√™n quan
- ‚ùå C√¢u tr·∫£ l·ªùi SAI HO√ÄN TO√ÄN ho·∫∑c HALLUCINATION (b·ªãa ƒë·∫∑t th√¥ng tin)

**0.5 ƒëi·ªÉm (L·ªói Format)**
- ‚ùå Thi·∫øu tr∆∞·ªùng b·∫Øt bu·ªôc
- ‚ùå Sai th·ª© t·ª± c√°c tr∆∞·ªùng
- ‚ùå Kh√¥ng kh·ªõp Regex pattern
(ƒêi·ªÉm n√†y ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω b·∫±ng pre-check, LLM kh√¥ng c·∫ßn ch·∫•m)

### QUY T·∫ÆC CH·∫§M ƒêI·ªÇM
1. **∆Øu ti√™n Accuracy**: N·∫øu c√¢u tr·∫£ l·ªùi SAI so v·ªõi Ground Truth ‚Üí t·ªëi ƒëa 2.0 ƒëi·ªÉm
2. **Logic quan tr·ªçng h∆°n chi ti·∫øt**: Sai Quy tr√¨nh/B∆∞·ªõc nghi√™m tr·ªçng h∆°n sai chi ti·∫øt nh·ªè
3. **Kh√¥ng khoan nh∆∞·ª£ng Hallucination**: B·ªãa ƒë·∫∑t th√¥ng tin ‚Üí t·ªëi ƒëa 1.5 ƒëi·ªÉm
4. **Tool call**: N·∫øu g·ªçi tool, ki·ªÉm tra tham s·ªë c√≥ ƒë√∫ng v·ªõi th√¥ng tin ƒë√£ c√≥ kh√¥ng

### V√ç D·ª§ CH·∫§M ƒêI·ªÇM

**V√≠ d·ª• 5.0 ƒëi·ªÉm:**
- C√¢u h·ªèi: "T√¥i l√† ch·ªß ƒëi·ªÉm b√°n, m√£ 305210, SƒêT 0909123456"
- Ground Truth: X√°c th·ª±c th√¥ng tin v√† h·ªó tr·ª£
- AI Thinking: "T√¨nh hu·ªëng: KH l√† ch·ªß ƒëi·ªÉm b√°n cung c·∫•p m√£ v√† SƒêT\nQuy tr√¨nh: Qu√™n/ƒê·ªïi m·∫≠t kh·∫©u\nB∆∞·ªõc: 1 - X√°c th·ª±c th√¥ng tin\n..."
- AI Response: G·ªçi tool tra_cuu_thong_tin v·ªõi m√£ 305210
‚Üí Logic ch√≠nh x√°c, h√†nh ƒë·ªông ƒë√∫ng

**V√≠ d·ª• 1.0 ƒëi·ªÉm:**
- C√¢u h·ªèi: "Heineken Ken c√≥ v·ªã g√¨?"
- Ground Truth: "Heineken c√≥ v·ªã ƒë·∫Øng nh·∫π, h∆∞∆°ng tr√°i c√¢y"
- AI Thinking: "T√¨nh hu·ªëng: KH ph√†n n√†n gi√° c·∫£\nQuy tr√¨nh: X·ª≠ l√Ω khi·∫øu n·∫°i\n..."
- AI Response: "Heineken c√≥ v·ªã ng·ªçt v√† m√†u v√†ng √≥ng"
‚Üí X√°c ƒë·ªãnh t√¨nh hu·ªëng SAI, c√¢u tr·∫£ l·ªùi HALLUCINATION

### L∆ØU √ù
- Ch·∫•m ƒëi·ªÉm NGHI√äM KH·∫ÆC nh∆∞ng c√¥ng b·∫±ng
- Reasoning ph·∫£i N√äU R√ï l√Ω do ch·∫•m ƒëi·ªÉm theo 3 ti√™u ch√≠: Format, Logic, Accuracy
- Kh√¥ng ch·∫•m ƒëi·ªÉm trung l·∫≠p (3.0) khi kh√¥ng ch·∫Øc ch·∫Øn ‚Üí Ph·∫£i c√≥ cƒÉn c·ª© r√µ r√†ng
                    """
                },
                {
                    "role": "user",
                    "content": (
                        f"# D·ªØ li·ªáu ƒë√°nh gi√°:\n"
                        f"- C√¢u h·ªèi kh√°ch h√†ng: {item['question']}\n"
                        f"- ƒê√°p √°n chu·∫©n (Ground Truth): {item['ground_truth']}\n"
                        f"- AI Suy lu·∫≠n (Thinking Part): {item['ai_thinking']}\n"
                        f"- AI Tr·∫£ l·ªùi (Response Part): {item['ai_response']}\n\n"
                        f"H√£y ph√¢n t√≠ch ng·∫Øn g·ªçn v√† ƒë∆∞a ra ƒëi·ªÉm s·ªë ch√≠nh x√°c."
                    )
                }
            ],
            response_format=QualityScore, # T√≠nh nƒÉng Structured Outputs native
        )
        
        # L·∫•y k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c parse th√†nh Pydantic object
        result: QualityScore = completion.choices[0].message.parsed
        
        # Debug nh·∫π ƒë·ªÉ xem model l√Ω lu·∫≠n th·∫ø n√†o (c√≥ th·ªÉ comment l·∫°i khi train th·∫≠t)
        # logger.info(f"Reasoning: {result.reasoning} | Score: {result.score}")
        
        return result.score

    except Exception as e:
        print(f"\n>>> ‚ùå L·ªñI G·ªåI API: {type(e).__name__}")
        print(f"Chi ti·∫øt: {e}")
        # N·∫øu l·ªói l√† AuthenticationError -> Sai key
        # N·∫øu l·ªói l√† AttributeError: '...' has no attribute 'parse' -> C·∫ßn update openai
        return 0.0

# --- 3. H√†m Reward Function Ch√≠nh ---
def judge_thinking_and_answer_alignment(prompts: List[Any], completions: List[Any], answer: List[Any], **kwargs) -> List[float]:
    """
    Reward function thay th·∫ø LangChain b·∫±ng OpenAI SDK + ThreadPoolExecutor.
    """
    
    # Regex t√°ch thinking
    split_pattern = r"(?:<think>|<thinking>)(.*?)(?:</think>|</thinking>)(.*)"
    
    batch_inputs = []

    # --- Pre-processing Data (Normalize inputs) ---
    normalized_completions = []
    for c in completions:
        if isinstance(c, list) and len(c) > 0:
            content = c[0].get('content', '') if isinstance(c[0], dict) else str(c[0])
        elif isinstance(c, dict):
            content = c.get('content') or c.get('generated_text', '')
        else:
            content = str(c)
        normalized_completions.append(content)

    for prompt, text, gt in zip(prompts, normalized_completions, answer):
        # L·∫•y c√¢u h·ªèi user
        user_q = prompt[-1]['content'] if isinstance(prompt, list) else str(prompt)

        # T√°ch thinking & response
        match = re.search(split_pattern, text, re.DOTALL | re.IGNORECASE)
        if match:
            thinking_part = match.group(1).strip()
            response_part = match.group(2).strip()
        else:
            thinking_part = "N/A (Missing <think> tag)"
            response_part = text
        
        batch_inputs.append({
            "question": user_q,
            "ground_truth": gt,
            "ai_thinking": thinking_part,
            "ai_response": response_part
        })

    # --- Execution: Ch·∫°y song song ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô ---
    # LangChain d√πng async/batch ng·∫ßm, v·ªõi SDK thu·∫ßn ta d√πng ThreadPoolExecutor
    scores = [0.0] * len(batch_inputs)
    
    with ThreadPoolExecutor(max_workers=5) as executor: # ƒêi·ªÅu ch·ªânh max_workers t√πy rate limit
        # Submit t·∫•t c·∫£ task v√† gi·ªØ map index ƒë·ªÉ tr·∫£ v·ªÅ ƒë√∫ng th·ª© t·ª±
        future_to_index = {executor.submit(_call_judge_single, item): i for i, item in enumerate(batch_inputs)}
        
        for future in as_completed(future_to_index):
            idx = future_to_index[future]
            try:
                scores[idx] = future.result()
            except Exception as exc:
                logger.error(f"Item {idx} generated an exception: {exc}")
                scores[idx] = 0.0

    return scores

import re
import numpy as np

# H√†m x·ª≠ l√Ω clean <think> v√† tokenize
def process_and_tokenize(examples):
    # 1. L·∫•y Raw Text ra tr∆∞·ªõc (ch∆∞a tokenize)
    texts = [tokenizer.apply_chat_template(p, add_generation_prompt=True, tokenize=False) for p in examples["prompt"]]
    
    # 2. D√πng Regex x√≥a b·ªè <think> (v√† kho·∫£ng tr·∫Øng th·ª´a) N·∫æU n√≥ n·∫±m ·ªü cu·ªëi c√πng
    # r'<think>\s*$' : T√¨m ch·ªØ <think> theo sau l√† kho·∫£ng tr·∫Øng b·∫•t k·ª≥ (\s*) ·ªü cu·ªëi chu·ªói ($)
    cleaned_texts = [re.sub(r'<think>\s*$', '', t).rstrip() for t in texts]
    
    # 3. Tokenize h√†ng lo·∫°t (nhanh h∆°n loop)
    # L∆∞u √Ω: add_special_tokens=False v√¨ chat_template th∆∞·ªùng ƒë√£ lo vi·ªác th√™m bos/eos r·ªìi
    tokens = tokenizer(cleaned_texts, add_special_tokens=False)["input_ids"]
    
    return {"tokens": tokens}

# √Åp d·ª•ng v√†o dataset
tokenized = train_dataset.map(
    process_and_tokenize,
    batched=True,
)

# --- PH·∫¶N KI·ªÇM TRA L·∫†I (QUAN TR·ªåNG) ---
print("--- Sample decoded check ---")
decoded_sample = tokenizer.decode(tokenized[0]["tokens"])
print(decoded_sample) 
# Ki·ªÉm tra xem cu·ªëi chu·ªói c√≥ b·ªã d√≠nh <think> kh√¥ng.
# N·∫øu ƒë√∫ng, n√≥ s·∫Ω k·∫øt th√∫c b·∫±ng header c·ªßa Assistant (vd: "<|im_start|>assistant")

# --- Ph·∫ßn t√≠nh to√°n Length gi·ªØ nguy√™n ---
tokenized = tokenized.map(lambda x: {"L": len(x["tokens"])})
maximum_length = int(np.quantile(tokenized["L"], 0.9))
print("Max Length =", maximum_length)

# Filter
train_dataset = train_dataset.select(np.where(np.array(tokenized["L"]) <= maximum_length)[0])
del tokenized

def filter_valid_records(example):
    return example['prompt'][-1]['role'] != 'assistant'

train_dataset = train_dataset.filter(filter_valid_records)
val_dataset = val_dataset.filter(filter_valid_records)

def format_multi_turn_to_tool_calling(example):
    messages = example['prompt']
    full_string = ""
    for msg in messages:
        role = msg['role']
        content = msg['content']
        if role == "system":
            full_string += f"<|im_start|>system\n{content}<|im_end|>\n"
        elif role == "user":
            full_string += f"<|im_start|>user\n{content}<|im_end|>\n"
        elif role in ["tool", "observation"]:
            full_string += f"<|im_start|>tool\n{content}<|im_end|>\n"
        elif role == "assistant":
            full_string += f"<|im_start|>assistant\n{content}<|im_end|>\n"
            
    # K·∫øt th√∫c b·∫±ng vi·ªác "m·ªìi" cho model chu·∫©n b·ªã suy nghƒ© v√† tr·∫£ l·ªùi
    full_string += "<|im_start|>assistant\n" 
    
    return {"prompt": full_string}

train_dataset = train_dataset.map(format_multi_turn_to_tool_calling)
val_dataset = val_dataset.map(format_multi_turn_to_tool_calling)

max_seq_length = 512 # Gi·∫£m t·ª´ 4096 xu·ªëng 2048 ƒë·ªÉ ti·∫øt ki·ªám VRAM
max_prompt_length = min(maximum_length + 1, 512)  # Cap prompt length
max_completion_length = max_seq_length - max_prompt_length

from vllm import SamplingParams
vllm_sampling_params = SamplingParams(
    min_p = 0.1,
    top_p = 1.0,
    top_k = -1,
    seed = 3407,
    stop = [tokenizer.eos_token],
    include_stop_str_in_output = True,
)

from trl import GRPOConfig, GRPOTrainer
training_args = GRPOConfig(
    torch_compile=False,
    vllm_sampling_params = vllm_sampling_params,
    temperature = 1.0,
    learning_rate = 5e-6,
    weight_decay = 0.001,
    warmup_ratio = 0.1,
    lr_scheduler_type = "linear",
    optim = "adamw_8bit",
    logging_steps = 1,
    
    # === MEMORY OPTIMIZATION ===
    per_device_train_batch_size = 1, 
    gradient_accumulation_steps = 4,  
    num_generations = 2,  
    
    max_prompt_length = max_prompt_length,
    max_completion_length = max_completion_length,
    # num_train_epochs = 1, # Set to 1 for a full training run
    max_steps = 50,
    save_steps = 5,
    report_to = "none", # Can use Weights & Biases
    output_dir = "outputs_qwen3_4B_grpo_qat",
    save_total_limit = 3,

    # For optional training + evaluation
    fp16_full_eval = True,
    per_device_eval_batch_size = 2, 
    eval_accumulation_steps = 1,
    eval_strategy = "steps",
    eval_steps = 5,

    metric_for_best_model="eval_loss", # Early stopping config
    fp16 = False,   
    bf16 = True,  
)

from transformers import EarlyStoppingCallback

trainer = GRPOTrainer(
    model = model,
    processing_class = tokenizer,
    reward_funcs = [
        match_format_exactly,
        judge_thinking_and_answer_alignment
    ],
    args = training_args,
    # For optional training + evaluation
    train_dataset = train_dataset,
    eval_dataset = val_dataset.select(range(1)), # Ch·ªâ eval 1 record ƒë·ªÉ ti·∫øt ki·ªám VRAM
)
early_stopping_callback = EarlyStoppingCallback(
    early_stopping_patience = 3,     # How many steps we will wait if the eval loss doesn't decrease
                                     # For example the loss might increase, but decrease after 3 steps
    early_stopping_threshold = 0.0,  # Can set higher - sets how much loss should decrease by until
                                     # we consider early stopping. For eg 0.01 means if loss was
                                     # 0.02 then 0.01, we consider to early stop the run.
)
trainer.add_callback(early_stopping_callback)
print(f"‚úÖ Process ƒëang ·ªü device {trainer.accelerator.device}")

if __name__ == "__main__":
    # === CLEAR MEMORY BEFORE TRAINING ===
    import gc
    gc.collect()
    torch.cuda.empty_cache()
    print(f"üßπ Cleared cache. VRAM before train: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
    
    trainer.train()